import os
from urllib.parse import urlparse
from profile_manager import ProfileManager
from llm_helper import (
    start_chat_session,
    get_conversation_response_stream,
    write_final_prompt_stream,
    evaluate_profile
)
from language_manager import lang_manager
from typing import Optional, Dict, Any, List
from web_scraper import web_scraper
from search_helper import search_helper

class ConversationHandler:
    """
    Orchestrates the conversation using a diagnostic-driven approach.
    æ”¯æŒç”¨æˆ·éš”ç¦»å’Œå¤šç”¨æˆ·ç³»ç»Ÿã€‚
    """
    def __init__(
        self, 
        session_id: Optional[str] = None,
        user_id: Optional[str] = None
    ):
        """
        åˆå§‹åŒ– ConversationHandler
        
        Args:
            session_id: ä¼šè¯ID
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºç”¨æˆ·éš”ç¦»ï¼‰
        """
        self.session_id = session_id
        self.user_id = user_id
        self.profile_manager = ProfileManager(
            session_id=session_id,
            user_id=user_id
        )
        self.chat_session = start_chat_session()
        self.last_critique = "è§’è‰²æ¡£æ¡ˆä¸ºç©ºï¼Œè¯·å¼•å¯¼ç”¨æˆ·æè¿°è§’è‰²çš„æ ¸å¿ƒèº«ä»½ã€‚"
        
        # å¦‚æœæ˜¯æ¢å¤çš„sessionï¼ŒåŠ è½½ä¹‹å‰çš„critique
        if session_id:
            self.load_session_state()
    
    def load_session_state(self):
        """Loads session state from metadata."""
        metadata = self.profile_manager.load_session_metadata()
        if metadata:
            # æ¢å¤æœ€åçš„critique
            evaluation_data = metadata.get("evaluation_data", {})
            self.last_critique = evaluation_data.get("last_critique", self.last_critique)
    
    def save_session_state(self):
        """Saves current session state to metadata."""
        self.profile_manager.update_session_metadata({
            "evaluation_data": {
                "last_critique": self.last_critique
            }
        })
        
    def get_initial_greeting(self):
        """
        Gets the initial greeting for the conversation.
        """
        initial_prompt = lang_manager.system_prompts.INITIAL_USER_PROMPT
        return self.handle_message(initial_prompt, is_initial=True)

    def handle_message(self, message: str, is_initial: bool = False):
        """
        Handles a user message, yields a stream of the LLM response, and then
        updates the profile based on the full response.
        """
        if not self.chat_session:
            yield lang_manager.t("ERROR_LLM_NOT_CONFIGURED")
            return
        original_message = message

        # 1. æ ¹æ®æœç´¢æ„å›¾è‡ªåŠ¨å†³å®šæ˜¯å¦è”ç½‘
        search_plan = search_helper.plan_search_strategy(original_message)
        if search_plan.get('should_search') and search_plan.get('query'):
            search_logs, enhanced_message = self._execute_search_plan(original_message, message, search_plan)
            for log in search_logs:
                yield log
            if enhanced_message:
                message = enhanced_message
        
        # 2. æ£€æŸ¥ç”¨æˆ·è¾“å…¥æ˜¯å¦åŒ…å«é“¾æ¥
        link_result = web_scraper.process_user_input(original_message)
        if link_result['has_url']:
            yield f"ğŸ”— æ£€æµ‹åˆ°é“¾æ¥: {link_result['url']}"
            
            if link_result['web_content'] and link_result['web_content']['success']:
                web_content = link_result['web_content']
                yield f"ğŸ“„ ç½‘é¡µæ ‡é¢˜: {web_content['title']}"
                
                if web_content['description']:
                    yield f"ğŸ“ ç½‘é¡µæè¿°: {web_content['description']}"
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºç½‘é¡µå†…å®¹é•¿åº¦
                content_length = len(web_content['content']) if web_content['content'] else 0
                yield f"ğŸ“Š ç½‘é¡µå†…å®¹é•¿åº¦: {content_length} å­—ç¬¦"
                
                # å°†ç½‘é¡µå†…å®¹æ•´åˆåˆ°æ¶ˆæ¯ä¸­
                enhanced_message = f"""
ç”¨æˆ·è¾“å…¥: {message}

ç½‘é¡µå†…å®¹:
æ ‡é¢˜: {web_content['title']}
æè¿°: {web_content['description']}
å†…å®¹: {web_content['content']}
å…³é”®è¯: {', '.join(web_content['keywords'])}

è¯·åŸºäºä»¥ä¸Šç½‘é¡µå†…å®¹å¸®åŠ©ç”¨æˆ·å®Œå–„è§’è‰²è®¾å®šã€‚
"""
                message = enhanced_message
                yield f"âœ… ç½‘é¡µå†…å®¹å·²æ•´åˆåˆ°ä¸Šä¸‹æ–‡ä¸­ï¼Œå†…å®¹é•¿åº¦: {content_length} å­—ç¬¦"
            else:
                error_msg = link_result.get('error', 'ç½‘é¡µæŠ“å–å¤±è´¥')
                yield f"âŒ ç½‘é¡µæŠ“å–å¤±è´¥: {error_msg}"
                # ç»§ç»­ä½¿ç”¨åŸå§‹æ¶ˆæ¯

        # On subsequent turns, first evaluate the profile to get a new critique
        if not is_initial:
            full_profile = self.profile_manager.get_full_profile()
            if full_profile:
                evaluation = evaluate_profile(full_profile)
                self.last_critique = evaluation.get("critique", self.last_critique)
                
                # ä¿å­˜sessionçŠ¶æ€
                self.save_session_state()
                
                if evaluation.get("is_ready_for_writing", False):
                    confirmation_reason = evaluation.get("critique", "è§’è‰²æ¡£æ¡ˆä¼¼ä¹å·²è¶³å¤Ÿå®Œæ•´ã€‚")
                    yield f"CONFIRM_GENERATION::{confirmation_reason}"
                    return

        # Get the generator for the streaming response
        response_generator = get_conversation_response_stream(self.chat_session, message, self.last_critique)
        
        # Handle the generator that yields chunks and final result
        full_response_chunks = []
        new_trait = "None"
        
        for chunk in response_generator:
            if isinstance(chunk, tuple) and len(chunk) == 3 and chunk[0] == "__FINAL_RESULT__":
                # This is the final result tuple
                _, ai_response, new_trait = chunk
                break
            yield chunk
            full_response_chunks.append(chunk)
        
        # Now that the stream is complete, append the new trait
        if new_trait and new_trait.lower() != "none":
            self.profile_manager.append_trait(new_trait)
        else:
            # å³ä½¿æ²¡æœ‰æå–åˆ°ç‰¹å¾ï¼Œä¹Ÿè¦è®°å½•ç”¨æˆ·è¾“å…¥ï¼Œç¡®ä¿è¯„ä¼°èƒ½å¤Ÿè§¦å‘
            # è¿™æ ·å¯ä»¥å¤„ç†ç½‘é¡µå†…å®¹ã€URLç­‰ç‰¹æ®Šè¾“å…¥
            self.profile_manager.append_trait(f"ç”¨æˆ·è¾“å…¥: {message}")
        
        # Signal that evaluation should start (will be handled separately in main.py)
        yield f"EVALUATION_TRIGGER::{lang_manager.t('EVALUATOR_EVALUATING')}"

    def finalize_prompt(self):
        """
        Finalizes the process by generating and saving the prompt.
        Yields the streaming content of the final prompt.
        """
        full_profile = self.profile_manager.get_full_profile()
        yield "\n" + lang_manager.t("FINAL_PROMPT_HEADER") + "\n"
        
        final_prompt_stream = write_final_prompt_stream(full_profile)
        final_prompt_content = ""
        for final_chunk in final_prompt_stream:
            yield final_chunk
            final_prompt_content += final_chunk
        
        self.profile_manager.save_final_prompt(final_prompt_content)
        yield "::FINAL_PROMPT_END::"

    def _execute_search_plan(self, original_message: str, current_message: str, plan: Dict[str, Any]):
        """Executes the resolved search plan and returns status logs plus an enhanced message."""
        logs: List[str] = []
        enhanced_message = None

        query = plan.get('query') or ''
        intent_type = plan.get('intent_type', 'concept')
        confidence = max(0.0, min(1.0, plan.get('confidence', 0.0)))
        reason = plan.get('reason') or ''

        confidence_pct = f"{confidence * 100:.0f}%"
        logs.append(f"ğŸŒ è”ç½‘æœç´¢è§¦å‘ï¼š{query} (ç½®ä¿¡åº¦ {confidence_pct})")
        if reason:
            logs.append(f"ğŸ“Œ è§¦å‘åŸå› : {reason}")

        if intent_type == 'character':
            logs.append("â³ æ­£åœ¨æœç´¢è§’è‰²ç›¸å…³çš„ wiki/ç™¾ç§‘èµ„æ–™...")
            search_data = search_helper.search_character_info(query)
            if search_data['success'] and search_data['search_results']:
                logs.append(f"\nğŸ“š æ‰¾åˆ° {len(search_data['search_results'])} ä¸ªä¿¡æ¯æ¥æº")

                character_details = search_data.get('character_details')
                if character_details:
                    if character_details.get('personality'):
                        logs.append(f"ğŸ’­ æ€§æ ¼: {character_details['personality'][:80]}...")
                    if character_details.get('background'):
                        logs.append(f"ğŸ“– èƒŒæ™¯: {character_details['background'][:80]}...")
                    if character_details.get('quotes'):
                        logs.append(f"ğŸ’¬ å°è¯: æ‰¾åˆ° {len(character_details['quotes'])} æ¡ç»å…¸å°è¯")

                enhanced = [
                    f"ç”¨æˆ·è¯¢é—®: {current_message}",
                    f"\nè§’è‰²åç§°: {query}\n"
                ]

                if character_details:
                    enhanced.append("=== è§’è‰²è¯¦ç»†ä¿¡æ¯ ===\n")
                    if character_details.get('background'):
                        enhanced.append(f"ã€èƒŒæ™¯æ•…äº‹ã€‘\n{character_details['background']}\n\n")
                    if character_details.get('personality'):
                        enhanced.append(f"ã€æ€§æ ¼ç‰¹å¾ã€‘\n{character_details['personality']}\n\n")
                    if character_details.get('appearance'):
                        enhanced.append(f"ã€å¤–è²Œç‰¹å¾ã€‘\n{character_details['appearance']}\n\n")
                    if character_details.get('abilities'):
                        enhanced.append(f"ã€èƒ½åŠ›æŠ€èƒ½ã€‘\n{character_details['abilities']}\n\n")
                    if character_details.get('quotes'):
                        enhanced.append("ã€ç»å…¸å°è¯ã€‘\n")
                        for idx, quote in enumerate(character_details['quotes'][:5], 1):
                            enhanced.append(f"{idx}. {quote}\n")
                        enhanced.append("\n")
                    if character_details.get('relationships'):
                        enhanced.append(f"ã€äººé™…å…³ç³»ã€‘\n{character_details['relationships']}\n\n")
                    if character_details.get('other_info'):
                        enhanced.append(f"ã€è¡¥å……ä¿¡æ¯ã€‘\n{character_details['other_info']}\n\n")

                enhanced.append("=== ä¿¡æ¯æ¥æº ===\n")
                for idx, result in enumerate(search_data['search_results'][:3], 1):
                    enhanced.append(f"{idx}. {result.get('title', 'æ— æ ‡é¢˜')}\n")
                    snippet = result.get('snippet')
                    if snippet:
                        enhanced.append(f"   {snippet[:150]}...\n")

                web_content = search_data.get('web_content')
                if web_content and web_content.get('success') and web_content.get('content'):
                    enhanced.append("\n=== ç½‘é¡µè¯¦ç»†å†…å®¹ ===\n")
                    enhanced.append(f"æ ‡é¢˜: {web_content.get('title', '')}\n")
                    enhanced.append(f"å†…å®¹: {web_content.get('content', '')[:1000]}\n")

                enhanced.append(
                    "\nè¯·åŸºäºä»¥ä¸Šæœç´¢åˆ°çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£è¿™ä¸ªè§’è‰²ã€‚\n"
                    "å¦‚æœç”¨æˆ·æƒ³åŸºäºè¿™ä¸ªè§’è‰²åˆ›å»ºpromptï¼Œè¯·å¼•å¯¼ç”¨æˆ·ç¡®è®¤æ˜¯å¦éœ€è¦ä¿®æ”¹æŸäº›è®¾å®šï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨è¿™äº›ä¿¡æ¯ã€‚\n"
                )
                enhanced_message = ''.join(enhanced)
                logs.append("âœ… æœç´¢å®Œæˆï¼Œå·²æå–è§’è‰²è¯¦ç»†ä¿¡æ¯")
            else:
                error_msg = search_data.get('error', 'æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯')
                logs.append(f"âš ï¸ æœç´¢ç»“æœæœ‰é™: {error_msg}")
                logs.append("ğŸ’¡ å°†å°è¯•åŸºäºç°æœ‰çŸ¥è¯†å›ç­”æ‚¨çš„é—®é¢˜")
        else:
            intent_label = 'æ¦‚å¿µ/äº‹å®' if intent_type == 'concept' else 'å®æ—¶èµ„è®¯'
            logs.append(f"ğŸŒ æ£€æµ‹åˆ°{intent_label}æŸ¥è¯¢: {query}")
            logs.append("â³ æ­£åœ¨è”ç½‘æ£€ç´¢ç›¸å…³èµ„æ–™...")

            concept_data = search_helper.search_concept_info(query)
            if concept_data['success']:
                summary = concept_data.get('concept_summary', '')
                key_points = concept_data.get('key_points', [])
                sources = concept_data.get('search_results', [])

                if summary:
                    snippet = summary[:160] + ("..." if len(summary) > 160 else "")
                    logs.append(f"ğŸ§  æ¦‚å¿µæ¦‚è¦: {snippet}")
                for idx, point in enumerate(key_points[:3], 1):
                    snippet = point[:160] + ("..." if len(point) > 160 else "")
                    logs.append(f"Â· å…³é”®è¦ç‚¹{idx}: {snippet}")

                if sources:
                    logs.append("ğŸ“š å‚è€ƒæ¥æº:")
                    for result in sources[:3]:
                        source_url = result.get('url', '')
                        domain = urlparse(source_url).netloc if source_url else ''
                        logs.append(f"  - {result.get('title', 'æ— æ ‡é¢˜')} ({domain})")

                enhanced = [
                    f"ç”¨æˆ·è¯¢é—®: {original_message}\n",
                    f"æ¦‚å¿µåç§°: {query}\n\n",
                    "=== æ¦‚å¿µæ¦‚è¦ ===\n",
                    f"{summary or 'æš‚æ— æƒå¨å®šä¹‰'}\n\n",
                    "=== å…³é”®è¦ç‚¹ ===\n"
                ]

                if key_points:
                    for point in key_points:
                        enhanced.append(f"- {point}\n")
                else:
                    enhanced.append("- æš‚æœªæå–åˆ°æ›´å¤šè¦ç‚¹\n")

                web_content = concept_data.get('web_content')
                if web_content and web_content.get('content'):
                    excerpt = web_content['content'][:1200]
                    enhanced.append("\n=== æ¥æºæ­£æ–‡æ‘˜å½• ===\n")
                    enhanced.append(excerpt + ("..." if len(web_content['content']) > len(excerpt) else ""))

                if sources:
                    enhanced.append("\n=== ä¿¡æ¯æ¥æº (Top 3) ===\n")
                    for result in sources[:3]:
                        source_url = result.get('url', '')
                        domain = urlparse(source_url).netloc if source_url else 'æœªçŸ¥æ¥æº'
                        enhanced.append(f"{result.get('title', 'æ— æ ‡é¢˜')} ({domain})\n")

                enhanced_message = ''.join(enhanced)
                logs.append("âœ… è”ç½‘èµ„æ–™å·²æ•´åˆåˆ°ä¸Šä¸‹æ–‡")
            else:
                error_msg = concept_data.get('error', 'æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™')
                logs.append(f"âš ï¸ æ¦‚å¿µæœç´¢å¤±è´¥: {error_msg}")

        return logs, enhanced_message