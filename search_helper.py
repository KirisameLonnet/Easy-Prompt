"""
Handles all interactions with the web search tool.
"""
import json
import re
import requests
from dataclasses import dataclass, field
from typing import Optional, Dict, Any, List
from web_scraper import web_scraper
from llm_helper import run_structured_prompt, get_current_api_type


@dataclass
class SearchIntent:
    """Represents the decision on whether a message should trigger web search."""

    should_search: bool
    intent_type: str = "concept"  # concept | character | fresh_news
    query: str = ""
    confidence: float = 0.0
    reason: str = ""
    focus_term: Optional[str] = None
    signals: Dict[str, Dict[str, Any]] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "should_search": self.should_search,
            "intent_type": self.intent_type,
            "query": self.query,
            "confidence": self.confidence,
            "reason": self.reason,
            "focus_term": self.focus_term,
            "signals": self.signals,
        }


class SearchHelper:
    """
    æœç´¢åŠ©æ‰‹ï¼Œç”¨äºæ£€æµ‹ç”¨æˆ·æŸ¥è¯¢æ„å›¾å¹¶æ‰§è¡Œç½‘ç»œæœç´¢
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        })

        # Precompiled keyword lists for intent detection
        self.explicit_search_words = [
            "æœç´¢", "æœä¸€ä¸‹", "æŸ¥ä¸€ä¸‹", "æŸ¥æŸ¥", "è”ç½‘", "ä¸Šç½‘æŸ¥", "å¸®æˆ‘æŸ¥", "å¸®æˆ‘æœ",
            "google", "ç™¾åº¦", "æ£€ç´¢", "look up", "search"
        ]
        self.uncertainty_words = [
            "ä¸çŸ¥é“", "ä¸æ¸…æ¥š", "ä¸ç†Ÿæ‚‰", "ä¸äº†è§£", "æ²¡å¬è¿‡", "å¿˜äº†", "æä¸æ‡‚",
            "ä¸æ˜ç™½", "not familiar", "no idea"
        ]
        self.assistant_probe_patterns = [
            r'(?:ä½ çŸ¥é“|ä½ äº†è§£|ä½ è®¤è¯†|å¬è¯´è¿‡).+å—',
            r'do you know [^?]+\?'
        ]
        self.time_sensitive_words = [
            "æœ€æ–°", "æœ€è¿‘", "ç°åœ¨", "å½“å‰", "å®æ—¶", "ä»Šå¹´", "åˆšåˆš", "news",
            "today", "update", "å‘ç”Ÿäº†ä»€ä¹ˆ", "æƒ…å†µå¦‚ä½•"
        ]
        self.verification_words = [
            "è¯æ®", "æ ¹æ®", "æ¥æº", "å‡ºå¤„", "å‚è€ƒ", "citation", "official", "å¯ä¿¡"
        ]
        self.fact_keywords = [
            "å†å²", "åŸç†", "èƒŒæ™¯", "ä¿¡æ¯", "èµ„æ–™", "äº‹å®", "ç»Ÿè®¡", "æƒ…å†µ", "æ•°æ®",
            "status", "evidence"
        ]
        self.info_words = [
            "ä»‹ç»", "è¯´æ˜", "è§£é‡Š", "å‘Šè¯‰", "èµ„æ–™", "èƒŒæ™¯", "æ¢—æ¦‚", "è¯¦æƒ…", "æ¦‚è¿°",
            "what is", "who is", "explain", "describe"
        ]
        self.definition_phrases = [
            "ä»€ä¹ˆæ˜¯", "å•¥æ˜¯", "ä½•è°“", "who is", "what is", "å®šä¹‰", "meaning of",
            "ä»€ä¹ˆæ„æ€", "æ˜¯ä»€ä¹ˆ", "æ˜¯å“ªä½", "which is", "å‘Šè¯‰æˆ‘å…³äº"
        ]
        self.character_keywords = [
            "è§’è‰²", "äººç‰©", "å¥³ä¸»", "ç”·ä¸»", "è‹±é›„", "åæ´¾", "character", "hero",
            "villain", "npc"
        ]
        self.news_keywords = [
            "æ–°é—»", "åŠ¨æ€", "å‘ç”Ÿ", "äº‹ä»¶", "çˆ†å‘", "update", "latest", "today",
            "ç°åœ¨æ€ä¹ˆæ ·", "ç°çŠ¶"
        ]
        self.creative_keywords = [
            "åˆ›å»º", "å†™ä¸€ä¸ª", "ç”Ÿæˆ", "è®¾è®¡", "ç¼–å†™", "åšä¸€ä¸ª", "write a", "create",
            "build", "ç”Ÿæˆä¸€ä¸ª", "è¯·å¸®æˆ‘å†™"
        ]
        self.stopwords_for_queries = {
            "è¿™ä¸ª", "é‚£ä¸ª", "è§’è‰²", "äººç‰©", "èµ„æ–™", "ä¿¡æ¯", "ä¸œè¥¿", "ä»€ä¹ˆ", "è¯·é—®",
            "ä¸€ä¸‹", "å…³äº", "ä»‹ç»", "æœ€æ–°", "æœ€è¿‘", "å¸®æˆ‘", "ä¸€ä¸ª", "æœ‰å“ªäº›", "æƒ…å†µ",
            "å‘ç”Ÿ", "å‘Šè¯‰", "æ•…äº‹", "å¦‚ä½•", "æ€ä¹ˆ", "æ€ä¹ˆæ ·", "æ€æ ·", "è¯·", "æƒ³è¦",
            "éœ€è¦", "ç”Ÿæˆ", "åˆ›é€ ", "åˆ›å»º", "è®¾è®¡", "å†™", "å†™ä¸ª", "å†™ä¸€æ®µ", "someone",
            "news", "today"
        }
        self.llm_planner_system_prompt = (
            "You are an assistant that decides whether to perform a real-time web search before "
            "answering a user. You must carefully analyze the user's message, determine if fresh, factual, or "
            "character-specific information from the internet is required, and output a strict JSON object with "
            "the following fields: should_search (bool), intent_type (concept|character|fresh_news), query "
            "(string), confidence (0-1 float), reason (short string), focus_term (string). Only respond with JSON."
        )

    def _add_signal(self, signals: Dict[str, Dict[str, Any]], key: str, weight: float, explanation: str):
        signals[key] = {"weight": weight, "explanation": explanation}

    def _collect_search_signals(self, message: str, focus_term: Optional[str]) -> Dict[str, Dict[str, Any]]:
        signals: Dict[str, Dict[str, Any]] = {}
        lowered = message.lower()

        if any(word in message for word in self.explicit_search_words):
            self._add_signal(signals, 'explicit_request', 3.2, "ç”¨æˆ·æ˜ç¡®è¦æ±‚è”ç½‘/æœç´¢")

        if any(word in message for word in self.uncertainty_words):
            self._add_signal(signals, 'knowledge_gap', 1.6, "ç”¨æˆ·è¡¨ç¤ºä¸ç†Ÿæ‚‰æˆ–ç¼ºä¹ä¿¡æ¯")

        for pattern in self.assistant_probe_patterns:
            if re.search(pattern, message, flags=re.IGNORECASE):
                self._add_signal(signals, 'knowledge_gap', 1.5, "ç”¨æˆ·ç¡®è®¤åŠ©æ‰‹æ˜¯å¦äº†è§£æŸå¯¹è±¡")
                break

        if any(word in message for word in self.time_sensitive_words) or re.search(r'20\d{2}', message):
            self._add_signal(signals, 'time_sensitive', 2.4, "é—®é¢˜æ¶‰åŠæœ€æ–°/æ—¶é—´æ•æ„Ÿä¿¡æ¯")

        if '?' in message or 'ï¼Ÿ' in message:
            self._add_signal(signals, 'question_form', 1.0, "è¾“å…¥å‘ˆç–‘é—®å¥å½¢å¼")

        if any(word in lowered for word in ['what is', 'who is', 'explain', 'tell me about', 'can you explain']):
            self._add_signal(signals, 'english_query', 1.4, "è‹±æ–‡ä¿¡æ¯æŸ¥è¯¢éœ€æ±‚")

        if any(word in message for word in self.verification_words):
            self._add_signal(signals, 'verification_need', 1.5, "ç”¨æˆ·è¦æ±‚æƒå¨æ¥æº/è¯æ®")

        if any(word in message for word in self.fact_keywords):
            self._add_signal(signals, 'factual_need', 1.2, "é—®é¢˜æ¶‰åŠå®¢è§‚èµ„æ–™")

        if any(word in message for word in self.info_words):
            self._add_signal(signals, 'info_request', 1.3, "ç”¨æˆ·è¯·æ±‚ç°æœ‰ä¿¡æ¯ç®€ä»‹")

        if any(phrase in message for phrase in self.definition_phrases):
            self._add_signal(signals, 'definition_request', 1.4, "ç”¨æˆ·åœ¨è¯¢é—®æ¦‚å¿µ/å®šä¹‰")

        if focus_term:
            self._add_signal(signals, 'specific_target', 1.1, f"æ£€æµ‹åˆ°å¯èƒ½çš„æŸ¥è¯¢å¯¹è±¡: {focus_term}")

        if any(word in message for word in self.creative_keywords):
            self._add_signal(signals, 'creative_only', -2.3, "è¾“å…¥ä»¥åˆ›ä½œ/ç”Ÿæˆéœ€æ±‚ä¸ºä¸»")

        return signals

    def _normalize_focus_term(self, text: str) -> str:
        if not text:
            return ""
        cleaned = text.strip()
        cleaned = cleaned.strip("ï¼Œ,ã€‚.?ï¼Ÿ!ï¼ï¼š:;Â·-~ ")
        for prefix in ['å…³äº', 'å¯¹äº', 'é’ˆå¯¹', 'ä»‹ç»', 'è¯´æ˜']:
            if cleaned.startswith(prefix) and len(cleaned) > len(prefix) + 1:
                cleaned = cleaned[len(prefix):]
        for suffix in ['è§’è‰²', 'äººç‰©', 'è®¾å®š', 'èµ„æ–™', 'ä¿¡æ¯']:
            if cleaned.endswith(suffix) and len(cleaned) > len(suffix) + 1:
                cleaned = cleaned[:-len(suffix)]
        return cleaned.strip()

    def _extract_focus_term(self, message: str) -> Optional[str]:
        if not message:
            return None

        search_patterns = [
            r'(?:è”ç½‘)?(?:æœç´¢|æœ|æŸ¥)(?:ä¸€ä¸‹|ä¸€ä¸‹ä¸‹|ä¸‹|ä¸€æ³¢)?(?P<term>[\u4e00-\u9fa5A-Za-z0-9Â·]{2,64})',
            r'look up (?P<term>[A-Za-z0-9\s\-]{2,64})',
            r'google (?P<term>[A-Za-z0-9\s\-]{2,64})'
        ]
        for pattern in search_patterns:
            match = re.search(pattern, message, flags=re.IGNORECASE)
            if match:
                term = self._normalize_focus_term(match.group('term'))
                if term:
                    return term

        ask_assistant_patterns = [
            r'(?:ä½ çŸ¥é“|ä½ äº†è§£|ä½ è®¤è¯†|å¬è¯´è¿‡)(?P<term>[\u4e00-\u9fa5A-Za-z0-9Â·]{2,32})(?:å—)?',
            r'do you know (?P<term>[A-Za-z0-9\s\-]{2,64})'
        ]
        for pattern in ask_assistant_patterns:
            match = re.search(pattern, message, flags=re.IGNORECASE)
            if match:
                term = self._normalize_focus_term(match.group('term'))
                if term:
                    return term

        quoted = re.search(r'[â€œ"ã€Šã€Œã€ã€(ï¼ˆ](?P<term>[\u4e00-\u9fa5A-Za-z0-9\s\-Â·]{2,64})[â€"ã€‹ã€ã€ã€‘)ï¼‰]', message)
        if quoted:
            term = self._normalize_focus_term(quoted.group('term'))
            if term:
                return term

        cn_patterns = [
            r'ä»€ä¹ˆæ˜¯(?P<term>[^?ï¼Ÿã€‚ï¼!]+)',
            r'(?P<term>[^?ï¼Ÿã€‚ï¼!]+?)æ˜¯ä»€ä¹ˆ',
            r'(?P<term>[^?ï¼Ÿã€‚ï¼!]+?)æ˜¯è°',
            r'ä»‹ç»(?:ä¸€ä¸‹)?(?P<term>[^?ï¼Ÿã€‚ï¼!]+)',
            r'å…³äº(?P<term>[^?ï¼Ÿã€‚ï¼!]+?)(?:çš„|æ˜¯|æœ‰)',
        ]
        for pattern in cn_patterns:
            match = re.search(pattern, message)
            if match:
                term = self._normalize_focus_term(match.group('term'))
                if term:
                    return term

        en_patterns = [
            r'what is (?P<term>[a-z0-9\s\-&]+)',
            r'who is (?P<term>[a-z0-9\s\-&]+)',
            r'can you explain (?P<term>[a-z0-9\s\-&]+)',
            r'tell me about (?P<term>[a-z0-9\s\-&]+)',
            r'explain (?P<term>[a-z0-9\s\-&]+)',
        ]
        lowered = message.lower()
        for pattern in en_patterns:
            match = re.search(pattern, lowered)
            if match:
                term = self._normalize_focus_term(match.group('term'))
                if term:
                    return term

        return None

    def _extract_chinese_candidates(self, message: str) -> List[str]:
        return re.findall(r'[\u4e00-\u9fa5]{2,8}', message)

    def _extract_english_candidates(self, message: str) -> List[str]:
        candidates = re.findall(r'[A-Za-z][A-Za-z0-9\s\-]{2,60}', message)
        return [cand.strip() for cand in candidates]

    def _derive_query_from_message(self, message: str, intent_type: str, focus_term: Optional[str]) -> str:
        if focus_term:
            return focus_term[:80]

        if intent_type == 'character':
            char_patterns = [
                r'(?P<name>[\u4e00-\u9fa5A-Za-z0-9Â·]{2,16})(?:è¿™ä¸ª|è¿™ä½|è¿™å)?(?:è§’è‰²|äººç‰©)',
                r'è§’è‰²(?P<name>[\u4e00-\u9fa5A-Za-z0-9Â·]{2,16})',
                r'character (?P<name>[A-Za-z0-9\s\-]{2,32})'
            ]
            for pattern in char_patterns:
                match = re.search(pattern, message, flags=re.IGNORECASE)
                if match:
                    candidate = self._normalize_focus_term(match.group('name'))
                    if candidate:
                        return candidate[:80]

        candidates: List[str] = []
        candidates.extend(self._extract_chinese_candidates(message))
        candidates.extend(self._extract_english_candidates(message))

        for candidate in candidates:
            normalized = self._normalize_focus_term(candidate)
            if normalized and normalized.lower() not in self.stopwords_for_queries:
                return normalized[:80]

        trimmed = re.sub(r'[?ï¼Ÿã€‚ï¼!ã€,:;]', ' ', message).strip()
        return trimmed[:80] if trimmed else message[:80]

    def _looks_like_name(self, term: Optional[str]) -> bool:
        if not term:
            return False
        has_chinese = bool(re.search(r'[\u4e00-\u9fa5]', term))
        if has_chinese and len(term) <= 6:
            return True
        if term and term[0].isupper():
            return True
        return False

    def _guess_intent_type(self, message: str, focus_term: Optional[str], signals: Dict[str, Dict[str, Any]]) -> str:
        lowered = message.lower()
        if any(word in message for word in self.news_keywords) or 'å‘ç”Ÿäº†' in message:
            return 'fresh_news'
        if any(word in message for word in self.character_keywords):
            return 'character'
        if focus_term and self._looks_like_name(focus_term):
            if any(word in message for word in ['è§’è‰²', 'äººç‰©', 'character']):
                return 'character'
            if 'explicit_request' in signals or 'knowledge_gap' in signals:
                return 'character'
        if any(word in lowered for word in ['latest', 'update', 'news']):
            return 'fresh_news'
        return 'concept'

    def plan_search_strategy(self, message: str) -> Dict[str, Any]:
        """Assess whether the assistant should perform a web search before answering."""
        content = (message or "").strip()
        if not content:
            return SearchIntent(False).to_dict()

        heuristic_intent = self._build_heuristic_intent(content)

        llm_plan = self._call_llm_planner(content, heuristic_intent)
        if llm_plan:
            return llm_plan.to_dict()
        return heuristic_intent.to_dict()

    def _build_heuristic_intent(self, content: str) -> SearchIntent:
        focus_term = self._extract_focus_term(content)
        signals = self._collect_search_signals(content, focus_term)
        intent_type = self._guess_intent_type(content, focus_term, signals)

        positive_score = sum(max(0.0, data['weight']) for data in signals.values())
        negative_score = sum(-min(0.0, data['weight']) for data in signals.values())
        net_score = positive_score - negative_score

        base_threshold = 2.4
        should_search = net_score >= base_threshold or 'explicit_request' in signals
        if 'time_sensitive' in signals:
            should_search = net_score >= 1.3 or 'explicit_request' in signals
        if 'creative_only' in signals and net_score < 3.5 and 'explicit_request' not in signals:
            should_search = False

        confidence = max(0.0, min(1.0, positive_score / 6.0))
        query = self._derive_query_from_message(content, intent_type, focus_term) if should_search else ""

        positive_reasons = [data['explanation'] for data in signals.values() if data['weight'] > 0]
        negative_reasons = [data['explanation'] for data in signals.values() if data['weight'] < 0]
        reason_parts = []
        if positive_reasons:
            reason_parts.append('ï¼›'.join(positive_reasons))
        if negative_reasons:
            reason_parts.append(f"æŠ‘åˆ¶æ¡ä»¶: {'ï¼›'.join(negative_reasons)}")
        reason = 'ï¼›'.join(reason_parts) if reason_parts else "æœªè§¦å‘è”ç½‘æ¡ä»¶"

        return SearchIntent(
            should_search=should_search,
            intent_type=intent_type,
            query=query,
            confidence=confidence,
            reason=reason,
            focus_term=focus_term,
            signals=signals
        )

    def _call_llm_planner(self, message: str, heuristic_intent: SearchIntent) -> Optional[SearchIntent]:
        if get_current_api_type() == "none":
            return None

        heuristic_payload = heuristic_intent.to_dict().copy()
        try:
            user_prompt = (
                "<UserMessage>\n"
                f"{message}\n"
                "</UserMessage>\n"
                "<HeuristicSuggestion>\n"
                f"{json.dumps(heuristic_payload, ensure_ascii=False)}\n"
                "</HeuristicSuggestion>\n"
                "è¯·åŸºäºç”¨æˆ·è¾“å…¥ä¸å¯å‘å¼å»ºè®®ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘æœç´¢ï¼Œå¹¶è¾“å‡ºä¸¥æ ¼çš„JSONã€‚"
            )
            response = run_structured_prompt(self.llm_planner_system_prompt, user_prompt)
            parsed = self._parse_planner_response(response)
            if not parsed:
                return None

            should_search = bool(parsed.get('should_search'))
            intent_type = parsed.get('intent_type', heuristic_intent.intent_type)
            query = parsed.get('query') or heuristic_intent.query
            focus_term = parsed.get('focus_term') or heuristic_intent.focus_term
            confidence = parsed.get('confidence', heuristic_intent.confidence)
            reason = parsed.get('reason') or heuristic_intent.reason

            if should_search and not query and focus_term:
                query = focus_term

            intent = SearchIntent(
                should_search=should_search,
                intent_type=intent_type or 'concept',
                query=(query or "")[:120],
                confidence=max(0.0, min(1.0, float(confidence) if confidence is not None else heuristic_intent.confidence)),
                reason=reason,
                focus_term=focus_term,
                signals=heuristic_intent.signals
            )
            return intent
        except Exception as exc:
            print(f"LLMæœç´¢è§„åˆ’å¤±è´¥: {exc}")
            return None

    def _parse_planner_response(self, response_text: str) -> Optional[Dict[str, Any]]:
        if not response_text:
            return None
        cleaned = response_text.strip()
        if cleaned.startswith("```"):
            cleaned = cleaned.strip('`')
            if cleaned.lower().startswith("json"):
                cleaned = cleaned[4:]
        cleaned = cleaned.strip()
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            return None
    
    def detect_character_query(self, message: str) -> Optional[Dict[str, Any]]:
        """æ–°ç‰ˆè§’è‰²æŸ¥è¯¢æ£€æµ‹ï¼ŒåŸºäºå¤šä¿¡å·æ„å›¾åˆ†æã€‚"""
        intent = self.plan_search_strategy(message)
        if intent['should_search'] and intent['intent_type'] == 'character' and intent['query']:
            character_name = self._normalize_focus_term(intent['query'])
            if not character_name:
                return None
            return {
                'is_query': True,
                'query_type': 'character_info',
                'character_name': character_name,
                'original_message': message,
                'confidence': intent.get('confidence', 0.0),
                'reason': intent.get('reason', '')
            }
        return None

    def detect_concept_query(self, message: str) -> Optional[Dict[str, Any]]:
        """æ–°ç‰ˆæ¦‚å¿µ/äº‹å®æŸ¥è¯¢æ£€æµ‹ï¼Œé¿å…ä¾èµ–å›ºå®šæ­£åˆ™ã€‚"""
        intent = self.plan_search_strategy(message)
        if intent['should_search'] and intent['intent_type'] in ('concept', 'fresh_news') and intent['query']:
            concept_name = self._normalize_focus_term(intent['query'])
            if not concept_name:
                return None
            return {
                'is_query': True,
                'query_type': 'concept_info',
                'concept_name': concept_name,
                'original_message': message,
                'confidence': intent.get('confidence', 0.0),
                'reason': intent.get('reason', '')
            }
        return None
    
    def search_duckduckgo(self, query: str, max_results: int = 3) -> Dict[str, Any]:
        """
        ä½¿ç”¨DuckDuckGoè¿›è¡Œæœç´¢
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            
        Returns:
            {
                'success': bool,
                'query': str,
                'results': List[Dict],  # æ¯ä¸ªç»“æœåŒ…å« title, url, snippet
                'error': Optional[str]
            }
        """
        try:
            # ä½¿ç”¨DuckDuckGo Instant Answer API
            api_url = 'https://api.duckduckgo.com/'
            params = {
                'q': query,
                'format': 'json',
                'pretty': 1,
                'no_html': 1,
                'skip_disambig': 1
            }
            
            print(f"æ­£åœ¨æœç´¢: {query}")
            response = self.session.get(api_url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            results = []
            
            # æå–æ‘˜è¦ä¿¡æ¯
            if data.get('AbstractText'):
                results.append({
                    'title': data.get('Heading', query),
                    'url': data.get('AbstractURL', ''),
                    'snippet': data.get('AbstractText', ''),
                    'source': data.get('AbstractSource', 'DuckDuckGo')
                })
            
            # æå–ç›¸å…³ä¸»é¢˜
            related_topics = data.get('RelatedTopics', [])
            for topic in related_topics[:max_results-1]:
                if isinstance(topic, dict) and 'Text' in topic:
                    results.append({
                        'title': topic.get('Text', '')[:100],
                        'url': topic.get('FirstURL', ''),
                        'snippet': topic.get('Text', ''),
                        'source': 'DuckDuckGo'
                    })
            
            # å¦‚æœDuckDuckGoæ²¡æœ‰ç›´æ¥ç»“æœï¼Œå°è¯•ä½¿ç”¨HTMLæœç´¢
            if not results:
                html_results = self._search_duckduckgo_html(query, max_results)
                if html_results['success']:
                    results = html_results['results']
            
            return {
                'success': True,
                'query': query,
                'results': results[:max_results],
                'error': None
            }
            
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return {
                'success': False,
                'query': query,
                'results': [],
                'error': str(e)
            }
    
    def _search_duckduckgo_html(self, query: str, max_results: int = 3) -> Dict[str, Any]:
        """
        ä½¿ç”¨DuckDuckGo HTMLæœç´¢ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
        """
        try:
            from bs4 import BeautifulSoup
            
            search_url = 'https://html.duckduckgo.com/html/'
            data = {'q': query}
            
            response = self.session.post(search_url, data=data, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # è§£ææœç´¢ç»“æœ
            result_divs = soup.find_all('div', class_='result')
            for div in result_divs[:max_results]:
                title_tag = div.find('a', class_='result__a')
                snippet_tag = div.find('a', class_='result__snippet')
                
                if title_tag:
                    results.append({
                        'title': title_tag.get_text().strip(),
                        'url': title_tag.get('href', ''),
                        'snippet': snippet_tag.get_text().strip() if snippet_tag else '',
                        'source': 'DuckDuckGo'
                    })
            
            return {
                'success': True,
                'query': query,
                'results': results,
                'error': None
            }
            
        except Exception as e:
            print(f"HTMLæœç´¢å¤±è´¥: {e}")
            return {
                'success': False,
                'query': query,
                'results': [],
                'error': str(e)
            }
    
    def _prioritize_wiki_sites(self, search_results: List[Dict]) -> List[Dict]:
        """
        å¯¹æœç´¢ç»“æœè¿›è¡Œä¼˜å…ˆçº§æ’åºï¼Œwiki/ç™¾ç§‘ç±»ç½‘ç«™ä¼˜å…ˆ
        
        ä¼˜å…ˆçº§é¡ºåºï¼š
        1. èŒå¨˜ç™¾ç§‘ (moegirl.org.cn)
        2. ç»´åŸºç™¾ç§‘ (wikipedia.org)
        3. Fandom Wiki (fandom.com)
        4. ç™¾åº¦ç™¾ç§‘ (baike.baidu.com)
        5. å…¶ä»–ç™¾ç§‘ç±»ç½‘ç«™
        6. å…¶ä»–ç½‘ç«™
        """
        # å®šä¹‰ä¼˜å…ˆçº§æƒé‡
        priority_domains = [
            ('moegirl.org', 100),      # èŒå¨˜ç™¾ç§‘
            ('wikipedia.org', 90),      # ç»´åŸºç™¾ç§‘
            ('fandom.com', 85),         # Fandom Wiki
            ('wiki.biligame.com', 80),  # å“”å“©å“”å“©æ¸¸æˆWiki
            ('baike.baidu.com', 70),    # ç™¾åº¦ç™¾ç§‘
            ('hudong.com', 60),         # äº’åŠ¨ç™¾ç§‘
        ]
        
        def get_priority(url: str) -> int:
            """è®¡ç®—URLçš„ä¼˜å…ˆçº§åˆ†æ•°"""
            if not url:
                return 0
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¼˜å…ˆåŸŸå
            for domain, score in priority_domains:
                if domain in url.lower():
                    return score
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯wikiç±»ç½‘ç«™
            if 'wiki' in url.lower():
                return 50
            
            # é»˜è®¤ä¼˜å…ˆçº§
            return 10
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_results = sorted(
            search_results, 
            key=lambda x: get_priority(x.get('url', '')), 
            reverse=True
        )
        
        return sorted_results
    
    def _extract_character_details(self, web_content: Dict[str, Any], character_name: str) -> Dict[str, Any]:
        """
        ä»ç½‘é¡µå†…å®¹ä¸­æå–è§’è‰²è¯¦ç»†ä¿¡æ¯ï¼ˆæ€§æ ¼ã€å°è¯ã€èƒŒæ™¯ç­‰ï¼‰
        
        Returns:
            {
                'personality': str,      # æ€§æ ¼ç‰¹å¾
                'quotes': List[str],     # ç»å…¸å°è¯
                'background': str,       # èƒŒæ™¯æ•…äº‹
                'appearance': str,       # å¤–è²Œç‰¹å¾
                'relationships': str,    # äººé™…å…³ç³»
                'abilities': str,        # èƒ½åŠ›/æŠ€èƒ½
                'other_info': str       # å…¶ä»–ä¿¡æ¯
            }
        """
        if not web_content or not web_content.get('success'):
            return {}
        
        content = web_content.get('content', '')
        title = web_content.get('title', '')
        
        details = {
            'personality': '',
            'quotes': [],
            'background': '',
            'appearance': '',
            'relationships': '',
            'abilities': '',
            'other_info': ''
        }
        
        # ä½¿ç”¨å…³é”®è¯æå–ç›¸å…³æ®µè½
        keywords_map = {
            'personality': ['æ€§æ ¼', 'ä¸ªæ€§', 'ç‰¹ç‚¹', 'è„¾æ°”', 'æ€§æƒ…'],
            'quotes': ['å°è¯', 'è¯­å½•', 'åè¨€', 'å£å¤´ç¦…', 'è¯´è¯'],
            'background': ['èƒŒæ™¯', 'ç»å†', 'æ•…äº‹', 'ç”Ÿå¹³', 'æ¥å†', 'èº«ä¸–'],
            'appearance': ['å¤–è²Œ', 'å½¢è±¡', 'å¤–è§‚', 'é•¿ç›¸', 'æ ·è²Œ', 'ç‰¹å¾'],
            'relationships': ['å…³ç³»', 'äººé™…', 'æœ‹å‹', 'å®¶äºº', 'åŒä¼´'],
            'abilities': ['èƒ½åŠ›', 'æŠ€èƒ½', 'ç‰¹æŠ€', 'æ‹›å¼', 'æŠ€å·§', 'å…ƒç´ '],
        }
        
        # åˆ†æ®µå¤„ç†å†…å®¹
        sentences = content.split('ã€‚')
        
        for category, keywords in keywords_map.items():
            relevant_sentences = []
            for sentence in sentences:
                if any(kw in sentence for kw in keywords):
                    # æ¸…ç†å¥å­
                    clean_sentence = sentence.strip()
                    if clean_sentence and len(clean_sentence) > 5:
                        relevant_sentences.append(clean_sentence)
            
            if relevant_sentences:
                if category == 'quotes':
                    # å°è¯å•ç‹¬å¤„ç†ä¸ºåˆ—è¡¨
                    details[category] = relevant_sentences[:5]  # æœ€å¤š5æ¡å°è¯
                else:
                    # å…¶ä»–ä¿¡æ¯åˆå¹¶
                    details[category] = 'ã€‚'.join(relevant_sentences[:3])  # æœ€å¤š3å¥è¯
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•ä¿¡æ¯ï¼Œä½¿ç”¨å…¨æ–‡å‰500å­—ä½œä¸ºåŸºç¡€ä¿¡æ¯
        if not any(details.values()):
            details['other_info'] = content[:500]
        
        return details

    def _extract_concept_highlights(self, content: str) -> Dict[str, Any]:
        """ä»ç½‘é¡µæ­£æ–‡ä¸­æå–æ¦‚å¿µæ¦‚è¦å’Œå…³é”®è¦ç‚¹"""
        if not content:
            return {'definition': '', 'key_points': []}

        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ!?\.]\s*', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]

        if not sentences:
            return {'definition': content[:200], 'key_points': []}

        definition = sentences[0]

        keyword_groups = [
            ['åº”ç”¨', 'ç”¨é€”', 'ä½¿ç”¨', 'åœºæ™¯'],
            ['ç‰¹ç‚¹', 'ç‰¹å¾', 'ä¼˜åŠ¿', 'åŠ£åŠ¿'],
            ['èµ·æº', 'å†å²', 'èƒŒæ™¯'],
            ['æ³¨æ„', 'é£é™©', 'é™åˆ¶'],
        ]

        key_points: List[str] = []
        for keywords in keyword_groups:
            for sentence in sentences[1:]:
                if any(keyword in sentence for keyword in keywords) and sentence not in key_points:
                    key_points.append(sentence)
                    break
            if len(key_points) >= 4:
                break

        # å¦‚æœå…³é”®è¯åŒ¹é…ä¸è¶³ï¼Œè¡¥å……å‰å‡ å¥
        if len(key_points) < 3:
            for sentence in sentences[1:6]:
                if sentence not in key_points:
                    key_points.append(sentence)
                if len(key_points) >= 4:
                    break

        return {
            'definition': definition,
            'key_points': key_points[:4]
        }
    
    def search_character_info(self, character_name: str) -> Dict[str, Any]:
        """
        æœç´¢è§’è‰²ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨wiki/ç™¾ç§‘ç±»ç½‘ç«™
        
        Args:
            character_name: è§’è‰²åç§°
            
        Returns:
            {
                'success': bool,
                'character_name': str,
                'search_results': List[Dict],
                'web_content': Optional[Dict],  # æœ€ä½³æœç´¢ç»“æœçš„ç½‘é¡µå†…å®¹
                'character_details': Optional[Dict],  # æå–çš„è§’è‰²è¯¦ç»†ä¿¡æ¯
                'error': Optional[str]
            }
        """
        # æ„å»ºå¤šä¸ªæœç´¢æŸ¥è¯¢ï¼Œæé«˜æœç´¢è´¨é‡
        search_queries = [
            f"{character_name} èŒå¨˜ç™¾ç§‘",
            f"{character_name} ç»´åŸºç™¾ç§‘",
            f"{character_name} è§’è‰²è®¾å®š",
        ]
        
        all_results = []
        
        # æ‰§è¡Œå¤šæ¬¡æœç´¢ï¼Œæ”¶é›†ç»“æœ
        for query in search_queries:
            search_result = self.search_duckduckgo(query, max_results=2)
            if search_result['success'] and search_result['results']:
                all_results.extend(search_result['results'])
        
        # å»é‡ï¼ˆåŸºäºURLï¼‰
        seen_urls = set()
        unique_results = []
        for result in all_results:
            url = result.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        if not unique_results:
            return {
                'success': False,
                'character_name': character_name,
                'search_results': [],
                'web_content': None,
                'character_details': None,
                'error': 'æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ'
            }
        
        # ä¼˜å…ˆé€‰æ‹©wiki/ç™¾ç§‘ç±»ç½‘ç«™
        prioritized_results = self._prioritize_wiki_sites(unique_results)
        
        print(f"æœç´¢åˆ° {len(prioritized_results)} ä¸ªç»“æœï¼Œä¼˜å…ˆå°è¯•wiki/ç™¾ç§‘ç½‘ç«™...")
        
        # å°è¯•æŠ“å–æœ€ä½³ç»“æœçš„ç½‘é¡µå†…å®¹
        web_content = None
        character_details = None
        
        for i, result in enumerate(prioritized_results[:3]):  # å°è¯•å‰3ä¸ªç»“æœ
            url = result.get('url', '')
            if url and url.startswith('http'):
                try:
                    print(f"å°è¯•æŠ“å–ç¬¬ {i+1} ä¸ªç»“æœ: {url}")
                    content = web_scraper.scrape_webpage(url)
                    
                    if content and content.get('success') and content.get('content'):
                        web_content = content
                        # æå–è§’è‰²è¯¦ç»†ä¿¡æ¯
                        character_details = self._extract_character_details(content, character_name)
                        print(f"âœ… æˆåŠŸæŠ“å–å¹¶æå–ä¿¡æ¯: {content.get('title', 'æœªçŸ¥')}")
                        break  # æ‰¾åˆ°æœ‰æ•ˆå†…å®¹å°±åœæ­¢
                    else:
                        print(f"âš ï¸ å†…å®¹ä¸ºç©ºæˆ–æŠ“å–å¤±è´¥")
                except Exception as e:
                    print(f"æŠ“å–å¤±è´¥: {e}")
                    continue
        
        return {
            'success': True,
            'character_name': character_name,
            'search_results': prioritized_results[:5],  # è¿”å›å‰5ä¸ªç»“æœ
            'web_content': web_content,
            'character_details': character_details,
            'error': None
        }

    def search_concept_info(self, concept_name: str) -> Dict[str, Any]:
        """æœç´¢é€šç”¨æ¦‚å¿µ/æœ¯è¯­çš„ä¿¡æ¯"""
        search_queries = [
            concept_name,
            f"{concept_name} æ˜¯ä»€ä¹ˆ",
            f"{concept_name} æ„ä¹‰",
            f"{concept_name} ç”¨é€”"
        ]

        aggregated_results: List[Dict[str, Any]] = []
        for query in search_queries:
            search_result = self.search_duckduckgo(query, max_results=2)
            if search_result['success'] and search_result['results']:
                aggregated_results.extend(search_result['results'])

        seen_urls = set()
        unique_results = []
        for result in aggregated_results:
            url = result.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)

        if not unique_results:
            return {
                'success': False,
                'concept_name': concept_name,
                'search_results': [],
                'web_content': None,
                'concept_summary': '',
                'key_points': [],
                'error': 'æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ'
            }

        prioritized_results = self._prioritize_wiki_sites(unique_results)

        web_content = None
        highlights = {'definition': '', 'key_points': []}

        for i, result in enumerate(prioritized_results[:3]):
            url = result.get('url', '')
            if url and url.startswith('http'):
                try:
                    content = web_scraper.scrape_webpage(url)
                    if content and content.get('success') and content.get('content'):
                        web_content = content
                        highlights = self._extract_concept_highlights(content.get('content', ''))
                        break
                except Exception as exc:
                    print(f"æ¦‚å¿µæœç´¢æŠ“å–å¤±è´¥({i+1}): {exc}")
                    continue

        if not highlights['definition'] and prioritized_results:
            highlights['definition'] = prioritized_results[0].get('snippet', '')[:200]

        return {
            'success': True,
            'concept_name': concept_name,
            'search_results': prioritized_results[:5],
            'web_content': web_content,
            'concept_summary': highlights.get('definition', ''),
            'key_points': highlights.get('key_points', []),
            'source_title': (web_content or {}).get('title'),
            'source_url': (web_content or {}).get('url'),
            'error': None
        }
    
    def format_search_results(self, search_data: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºå¯è¯»æ–‡æœ¬ï¼ŒåŒ…æ‹¬æå–çš„è§’è‰²è¯¦ç»†ä¿¡æ¯
        """
        if not search_data['success']:
            return f"âŒ æœç´¢å¤±è´¥: {search_data.get('error', 'æœªçŸ¥é”™è¯¯')}"
        
        character_name = search_data['character_name']
        search_results = search_data['search_results']
        web_content = search_data.get('web_content')
        character_details = search_data.get('character_details')
        
        # æ„å»ºæ ¼å¼åŒ–æ–‡æœ¬
        formatted_text = f"\nğŸ” æœç´¢è§’è‰²: {character_name}\n\n"
        
        # æ·»åŠ è§’è‰²è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰æå–åˆ°ï¼‰
        if character_details:
            formatted_text += "ğŸ“‹ è§’è‰²è¯¦ç»†ä¿¡æ¯:\n\n"
            
            if character_details.get('background'):
                formatted_text += f"ğŸ“– èƒŒæ™¯æ•…äº‹:\n{character_details['background']}\n\n"
            
            if character_details.get('personality'):
                formatted_text += f"ğŸ’­ æ€§æ ¼ç‰¹å¾:\n{character_details['personality']}\n\n"
            
            if character_details.get('appearance'):
                formatted_text += f"ğŸ‘¤ å¤–è²Œç‰¹å¾:\n{character_details['appearance']}\n\n"
            
            if character_details.get('abilities'):
                formatted_text += f"âš¡ èƒ½åŠ›æŠ€èƒ½:\n{character_details['abilities']}\n\n"
            
            if character_details.get('quotes') and len(character_details['quotes']) > 0:
                formatted_text += f"ğŸ’¬ ç»å…¸å°è¯:\n"
                for i, quote in enumerate(character_details['quotes'][:3], 1):
                    formatted_text += f"  {i}. {quote}\n"
                formatted_text += "\n"
            
            if character_details.get('relationships'):
                formatted_text += f"ğŸ‘¥ äººé™…å…³ç³»:\n{character_details['relationships']}\n\n"
            
            if character_details.get('other_info') and not any([
                character_details.get('background'),
                character_details.get('personality'),
                character_details.get('appearance')
            ]):
                formatted_text += f"â„¹ï¸ å…¶ä»–ä¿¡æ¯:\n{character_details['other_info'][:300]}...\n\n"
        
        # æ·»åŠ æœç´¢æ¥æº
        if search_results:
            formatted_text += "ğŸ“š ä¿¡æ¯æ¥æº:\n"
            for i, result in enumerate(search_results[:3], 1):
                formatted_text += f"{i}. {result.get('title', 'æ— æ ‡é¢˜')}\n"
                if result.get('url'):
                    formatted_text += f"   ğŸ”— {result['url']}\n"
        
        # æ·»åŠ ç½‘é¡µå†…å®¹ä¿¡æ¯
        if web_content and web_content.get('success'):
            formatted_text += f"\nğŸ“„ ä¸»è¦æ¥æº: {web_content.get('title', 'æœªçŸ¥')}\n"
        
        formatted_text += "\nâœ… æœç´¢å®Œæˆï¼Œè¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç»§ç»­å®Œå–„è§’è‰²è®¾å®šã€‚\n"
        
        return formatted_text


# å…¨å±€å®ä¾‹
search_helper = SearchHelper()
