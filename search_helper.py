"""
Handles all interactions with the web search tool.
"""
import re
import requests
from typing import Optional, Dict, Any, List
from web_scraper import web_scraper


class SearchHelper:
    """
    æœç´¢åŠ©æ‰‹ï¼Œç”¨äºæ£€æµ‹ç”¨æˆ·æŸ¥è¯¢æ„å›¾å¹¶æ‰§è¡Œç½‘ç»œæœç´¢
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        })
    
    def detect_character_query(self, message: str) -> Optional[Dict[str, Any]]:
        """
        æ£€æµ‹ç”¨æˆ·æ˜¯å¦åœ¨è¯¢é—®è§’è‰²/äººç‰©ä¿¡æ¯
        
        Returns:
            å¦‚æœæ˜¯è¯¢é—®è§’è‰²ï¼Œè¿”å› {
                'is_query': True,
                'query_type': 'character_info',
                'character_name': str,
                'original_message': str
            }
            å¦åˆ™è¿”å› None
        """
        # è¯¢é—®è§’è‰²çš„å¸¸è§æ¨¡å¼ï¼ˆä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼ï¼‰
        patterns = [
            # "ä½ çŸ¥é“é›·ç”µå°†å†›è¿™ä¸ªè§’è‰²å—"
            (r'(?:ä½ çŸ¥é“|ä½ äº†è§£|ä½ è®¤è¯†|å¬è¯´è¿‡)(?:å—)?(.+?)(?:è¿™ä¸ª)?(?:è§’è‰²|äººç‰©)', 1),
            # "å…³äºå­™æ‚Ÿç©ºè§’è‰²çš„ä¿¡æ¯" - æå–"å­™æ‚Ÿç©º"
            (r'å…³äº(.+?)(?:è¿™ä¸ª)?(?:è§’è‰²|äººç‰©)', 1),
            # "è§’è‰²é¸£äººæ˜¯ä»€ä¹ˆæ ·çš„"
            (r'è§’è‰²(.+?)(?:æ˜¯ä»€ä¹ˆ|æ˜¯è°|æ€ä¹ˆæ ·|å¦‚ä½•)', 1),
            # "ä»‹ç»ä¸€ä¸‹è‰¾æ–¯è¿™ä¸ªè§’è‰²"
            (r'ä»‹ç»(?:ä¸€ä¸‹)?(.+?)(?:è¿™ä¸ª)?(?:è§’è‰²|äººç‰©)', 1),
            # "é›·ç”µå°†å†›è¿™ä¸ªè§’è‰²çš„èƒŒæ™¯"
            (r'(.+?)(?:è¿™ä¸ª)?(?:è§’è‰²|äººç‰©)(?:çš„)?(?:èƒŒæ™¯|è®¾å®š|ä¿¡æ¯|èµ„æ–™)', 1),
        ]
        
        # éœ€è¦æ’é™¤çš„æ— æ•ˆè¯
        invalid_words = ['è¿™ä¸ª', 'é‚£ä¸ª', 'æŸä¸ª', 'ä¸€ä¸ª', 'æˆ‘æƒ³åˆ›å»º', 'åˆ›å»º', 'æ–°å»º', 'å…³äº']
        
        for pattern, group_idx in patterns:
            match = re.search(pattern, message)
            if match:
                character_name = match.group(group_idx).strip()
                
                # æ¸…ç†è§’è‰²åç§°
                # ç§»é™¤å¼€å¤´çš„"å…³äº"ç­‰è¯
                for word in ['å…³äº', 'å¯¹äº', 'é’ˆå¯¹']:
                    if character_name.startswith(word):
                        character_name = character_name[len(word):].strip()
                
                # éªŒè¯è§’è‰²åç§°æœ‰æ•ˆæ€§
                is_valid = (
                    character_name and 
                    len(character_name) > 1 and 
                    character_name not in invalid_words and
                    not any(invalid in character_name for invalid in ['åˆ›å»º', 'æ–°å»º'])
                )
                
                if is_valid:
                    return {
                        'is_query': True,
                        'query_type': 'character_info',
                        'character_name': character_name,
                        'original_message': message
                    }
        
        return None
    
    def search_duckduckgo(self, query: str, max_results: int = 3) -> Dict[str, Any]:
        """
        ä½¿ç”¨DuckDuckGoè¿›è¡Œæœç´¢
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            
        Returns:
            {
                'success': bool,
                'query': str,
                'results': List[Dict],  # æ¯ä¸ªç»“æœåŒ…å« title, url, snippet
                'error': Optional[str]
            }
        """
        try:
            # ä½¿ç”¨DuckDuckGo Instant Answer API
            api_url = 'https://api.duckduckgo.com/'
            params = {
                'q': query,
                'format': 'json',
                'pretty': 1,
                'no_html': 1,
                'skip_disambig': 1
            }
            
            print(f"æ­£åœ¨æœç´¢: {query}")
            response = self.session.get(api_url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            results = []
            
            # æå–æ‘˜è¦ä¿¡æ¯
            if data.get('AbstractText'):
                results.append({
                    'title': data.get('Heading', query),
                    'url': data.get('AbstractURL', ''),
                    'snippet': data.get('AbstractText', ''),
                    'source': data.get('AbstractSource', 'DuckDuckGo')
                })
            
            # æå–ç›¸å…³ä¸»é¢˜
            related_topics = data.get('RelatedTopics', [])
            for topic in related_topics[:max_results-1]:
                if isinstance(topic, dict) and 'Text' in topic:
                    results.append({
                        'title': topic.get('Text', '')[:100],
                        'url': topic.get('FirstURL', ''),
                        'snippet': topic.get('Text', ''),
                        'source': 'DuckDuckGo'
                    })
            
            # å¦‚æœDuckDuckGoæ²¡æœ‰ç›´æ¥ç»“æœï¼Œå°è¯•ä½¿ç”¨HTMLæœç´¢
            if not results:
                html_results = self._search_duckduckgo_html(query, max_results)
                if html_results['success']:
                    results = html_results['results']
            
            return {
                'success': True,
                'query': query,
                'results': results[:max_results],
                'error': None
            }
            
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return {
                'success': False,
                'query': query,
                'results': [],
                'error': str(e)
            }
    
    def _search_duckduckgo_html(self, query: str, max_results: int = 3) -> Dict[str, Any]:
        """
        ä½¿ç”¨DuckDuckGo HTMLæœç´¢ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
        """
        try:
            from bs4 import BeautifulSoup
            
            search_url = 'https://html.duckduckgo.com/html/'
            data = {'q': query}
            
            response = self.session.post(search_url, data=data, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # è§£ææœç´¢ç»“æœ
            result_divs = soup.find_all('div', class_='result')
            for div in result_divs[:max_results]:
                title_tag = div.find('a', class_='result__a')
                snippet_tag = div.find('a', class_='result__snippet')
                
                if title_tag:
                    results.append({
                        'title': title_tag.get_text().strip(),
                        'url': title_tag.get('href', ''),
                        'snippet': snippet_tag.get_text().strip() if snippet_tag else '',
                        'source': 'DuckDuckGo'
                    })
            
            return {
                'success': True,
                'query': query,
                'results': results,
                'error': None
            }
            
        except Exception as e:
            print(f"HTMLæœç´¢å¤±è´¥: {e}")
            return {
                'success': False,
                'query': query,
                'results': [],
                'error': str(e)
            }
    
    def _prioritize_wiki_sites(self, search_results: List[Dict]) -> List[Dict]:
        """
        å¯¹æœç´¢ç»“æœè¿›è¡Œä¼˜å…ˆçº§æ’åºï¼Œwiki/ç™¾ç§‘ç±»ç½‘ç«™ä¼˜å…ˆ
        
        ä¼˜å…ˆçº§é¡ºåºï¼š
        1. èŒå¨˜ç™¾ç§‘ (moegirl.org.cn)
        2. ç»´åŸºç™¾ç§‘ (wikipedia.org)
        3. Fandom Wiki (fandom.com)
        4. ç™¾åº¦ç™¾ç§‘ (baike.baidu.com)
        5. å…¶ä»–ç™¾ç§‘ç±»ç½‘ç«™
        6. å…¶ä»–ç½‘ç«™
        """
        # å®šä¹‰ä¼˜å…ˆçº§æƒé‡
        priority_domains = [
            ('moegirl.org', 100),      # èŒå¨˜ç™¾ç§‘
            ('wikipedia.org', 90),      # ç»´åŸºç™¾ç§‘
            ('fandom.com', 85),         # Fandom Wiki
            ('wiki.biligame.com', 80),  # å“”å“©å“”å“©æ¸¸æˆWiki
            ('baike.baidu.com', 70),    # ç™¾åº¦ç™¾ç§‘
            ('hudong.com', 60),         # äº’åŠ¨ç™¾ç§‘
        ]
        
        def get_priority(url: str) -> int:
            """è®¡ç®—URLçš„ä¼˜å…ˆçº§åˆ†æ•°"""
            if not url:
                return 0
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¼˜å…ˆåŸŸå
            for domain, score in priority_domains:
                if domain in url.lower():
                    return score
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯wikiç±»ç½‘ç«™
            if 'wiki' in url.lower():
                return 50
            
            # é»˜è®¤ä¼˜å…ˆçº§
            return 10
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_results = sorted(
            search_results, 
            key=lambda x: get_priority(x.get('url', '')), 
            reverse=True
        )
        
        return sorted_results
    
    def _extract_character_details(self, web_content: Dict[str, Any], character_name: str) -> Dict[str, Any]:
        """
        ä»ç½‘é¡µå†…å®¹ä¸­æå–è§’è‰²è¯¦ç»†ä¿¡æ¯ï¼ˆæ€§æ ¼ã€å°è¯ã€èƒŒæ™¯ç­‰ï¼‰
        
        Returns:
            {
                'personality': str,      # æ€§æ ¼ç‰¹å¾
                'quotes': List[str],     # ç»å…¸å°è¯
                'background': str,       # èƒŒæ™¯æ•…äº‹
                'appearance': str,       # å¤–è²Œç‰¹å¾
                'relationships': str,    # äººé™…å…³ç³»
                'abilities': str,        # èƒ½åŠ›/æŠ€èƒ½
                'other_info': str       # å…¶ä»–ä¿¡æ¯
            }
        """
        if not web_content or not web_content.get('success'):
            return {}
        
        content = web_content.get('content', '')
        title = web_content.get('title', '')
        
        details = {
            'personality': '',
            'quotes': [],
            'background': '',
            'appearance': '',
            'relationships': '',
            'abilities': '',
            'other_info': ''
        }
        
        # ä½¿ç”¨å…³é”®è¯æå–ç›¸å…³æ®µè½
        keywords_map = {
            'personality': ['æ€§æ ¼', 'ä¸ªæ€§', 'ç‰¹ç‚¹', 'è„¾æ°”', 'æ€§æƒ…'],
            'quotes': ['å°è¯', 'è¯­å½•', 'åè¨€', 'å£å¤´ç¦…', 'è¯´è¯'],
            'background': ['èƒŒæ™¯', 'ç»å†', 'æ•…äº‹', 'ç”Ÿå¹³', 'æ¥å†', 'èº«ä¸–'],
            'appearance': ['å¤–è²Œ', 'å½¢è±¡', 'å¤–è§‚', 'é•¿ç›¸', 'æ ·è²Œ', 'ç‰¹å¾'],
            'relationships': ['å…³ç³»', 'äººé™…', 'æœ‹å‹', 'å®¶äºº', 'åŒä¼´'],
            'abilities': ['èƒ½åŠ›', 'æŠ€èƒ½', 'ç‰¹æŠ€', 'æ‹›å¼', 'æŠ€å·§', 'å…ƒç´ '],
        }
        
        # åˆ†æ®µå¤„ç†å†…å®¹
        sentences = content.split('ã€‚')
        
        for category, keywords in keywords_map.items():
            relevant_sentences = []
            for sentence in sentences:
                if any(kw in sentence for kw in keywords):
                    # æ¸…ç†å¥å­
                    clean_sentence = sentence.strip()
                    if clean_sentence and len(clean_sentence) > 5:
                        relevant_sentences.append(clean_sentence)
            
            if relevant_sentences:
                if category == 'quotes':
                    # å°è¯å•ç‹¬å¤„ç†ä¸ºåˆ—è¡¨
                    details[category] = relevant_sentences[:5]  # æœ€å¤š5æ¡å°è¯
                else:
                    # å…¶ä»–ä¿¡æ¯åˆå¹¶
                    details[category] = 'ã€‚'.join(relevant_sentences[:3])  # æœ€å¤š3å¥è¯
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•ä¿¡æ¯ï¼Œä½¿ç”¨å…¨æ–‡å‰500å­—ä½œä¸ºåŸºç¡€ä¿¡æ¯
        if not any(details.values()):
            details['other_info'] = content[:500]
        
        return details
    
    def search_character_info(self, character_name: str) -> Dict[str, Any]:
        """
        æœç´¢è§’è‰²ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨wiki/ç™¾ç§‘ç±»ç½‘ç«™
        
        Args:
            character_name: è§’è‰²åç§°
            
        Returns:
            {
                'success': bool,
                'character_name': str,
                'search_results': List[Dict],
                'web_content': Optional[Dict],  # æœ€ä½³æœç´¢ç»“æœçš„ç½‘é¡µå†…å®¹
                'character_details': Optional[Dict],  # æå–çš„è§’è‰²è¯¦ç»†ä¿¡æ¯
                'error': Optional[str]
            }
        """
        # æ„å»ºå¤šä¸ªæœç´¢æŸ¥è¯¢ï¼Œæé«˜æœç´¢è´¨é‡
        search_queries = [
            f"{character_name} èŒå¨˜ç™¾ç§‘",
            f"{character_name} ç»´åŸºç™¾ç§‘",
            f"{character_name} è§’è‰²è®¾å®š",
        ]
        
        all_results = []
        
        # æ‰§è¡Œå¤šæ¬¡æœç´¢ï¼Œæ”¶é›†ç»“æœ
        for query in search_queries:
            search_result = self.search_duckduckgo(query, max_results=2)
            if search_result['success'] and search_result['results']:
                all_results.extend(search_result['results'])
        
        # å»é‡ï¼ˆåŸºäºURLï¼‰
        seen_urls = set()
        unique_results = []
        for result in all_results:
            url = result.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        if not unique_results:
            return {
                'success': False,
                'character_name': character_name,
                'search_results': [],
                'web_content': None,
                'character_details': None,
                'error': 'æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ'
            }
        
        # ä¼˜å…ˆé€‰æ‹©wiki/ç™¾ç§‘ç±»ç½‘ç«™
        prioritized_results = self._prioritize_wiki_sites(unique_results)
        
        print(f"æœç´¢åˆ° {len(prioritized_results)} ä¸ªç»“æœï¼Œä¼˜å…ˆå°è¯•wiki/ç™¾ç§‘ç½‘ç«™...")
        
        # å°è¯•æŠ“å–æœ€ä½³ç»“æœçš„ç½‘é¡µå†…å®¹
        web_content = None
        character_details = None
        
        for i, result in enumerate(prioritized_results[:3]):  # å°è¯•å‰3ä¸ªç»“æœ
            url = result.get('url', '')
            if url and url.startswith('http'):
                try:
                    print(f"å°è¯•æŠ“å–ç¬¬ {i+1} ä¸ªç»“æœ: {url}")
                    content = web_scraper.scrape_webpage(url)
                    
                    if content and content.get('success') and content.get('content'):
                        web_content = content
                        # æå–è§’è‰²è¯¦ç»†ä¿¡æ¯
                        character_details = self._extract_character_details(content, character_name)
                        print(f"âœ… æˆåŠŸæŠ“å–å¹¶æå–ä¿¡æ¯: {content.get('title', 'æœªçŸ¥')}")
                        break  # æ‰¾åˆ°æœ‰æ•ˆå†…å®¹å°±åœæ­¢
                    else:
                        print(f"âš ï¸ å†…å®¹ä¸ºç©ºæˆ–æŠ“å–å¤±è´¥")
                except Exception as e:
                    print(f"æŠ“å–å¤±è´¥: {e}")
                    continue
        
        return {
            'success': True,
            'character_name': character_name,
            'search_results': prioritized_results[:5],  # è¿”å›å‰5ä¸ªç»“æœ
            'web_content': web_content,
            'character_details': character_details,
            'error': None
        }
    
    def format_search_results(self, search_data: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºå¯è¯»æ–‡æœ¬ï¼ŒåŒ…æ‹¬æå–çš„è§’è‰²è¯¦ç»†ä¿¡æ¯
        """
        if not search_data['success']:
            return f"âŒ æœç´¢å¤±è´¥: {search_data.get('error', 'æœªçŸ¥é”™è¯¯')}"
        
        character_name = search_data['character_name']
        search_results = search_data['search_results']
        web_content = search_data.get('web_content')
        character_details = search_data.get('character_details')
        
        # æ„å»ºæ ¼å¼åŒ–æ–‡æœ¬
        formatted_text = f"\nğŸ” æœç´¢è§’è‰²: {character_name}\n\n"
        
        # æ·»åŠ è§’è‰²è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰æå–åˆ°ï¼‰
        if character_details:
            formatted_text += "ğŸ“‹ è§’è‰²è¯¦ç»†ä¿¡æ¯:\n\n"
            
            if character_details.get('background'):
                formatted_text += f"ğŸ“– èƒŒæ™¯æ•…äº‹:\n{character_details['background']}\n\n"
            
            if character_details.get('personality'):
                formatted_text += f"ğŸ’­ æ€§æ ¼ç‰¹å¾:\n{character_details['personality']}\n\n"
            
            if character_details.get('appearance'):
                formatted_text += f"ğŸ‘¤ å¤–è²Œç‰¹å¾:\n{character_details['appearance']}\n\n"
            
            if character_details.get('abilities'):
                formatted_text += f"âš¡ èƒ½åŠ›æŠ€èƒ½:\n{character_details['abilities']}\n\n"
            
            if character_details.get('quotes') and len(character_details['quotes']) > 0:
                formatted_text += f"ğŸ’¬ ç»å…¸å°è¯:\n"
                for i, quote in enumerate(character_details['quotes'][:3], 1):
                    formatted_text += f"  {i}. {quote}\n"
                formatted_text += "\n"
            
            if character_details.get('relationships'):
                formatted_text += f"ğŸ‘¥ äººé™…å…³ç³»:\n{character_details['relationships']}\n\n"
            
            if character_details.get('other_info') and not any([
                character_details.get('background'),
                character_details.get('personality'),
                character_details.get('appearance')
            ]):
                formatted_text += f"â„¹ï¸ å…¶ä»–ä¿¡æ¯:\n{character_details['other_info'][:300]}...\n\n"
        
        # æ·»åŠ æœç´¢æ¥æº
        if search_results:
            formatted_text += "ğŸ“š ä¿¡æ¯æ¥æº:\n"
            for i, result in enumerate(search_results[:3], 1):
                formatted_text += f"{i}. {result.get('title', 'æ— æ ‡é¢˜')}\n"
                if result.get('url'):
                    formatted_text += f"   ğŸ”— {result['url']}\n"
        
        # æ·»åŠ ç½‘é¡µå†…å®¹ä¿¡æ¯
        if web_content and web_content.get('success'):
            formatted_text += f"\nğŸ“„ ä¸»è¦æ¥æº: {web_content.get('title', 'æœªçŸ¥')}\n"
        
        formatted_text += "\nâœ… æœç´¢å®Œæˆï¼Œè¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç»§ç»­å®Œå–„è§’è‰²è®¾å®šã€‚\n"
        
        return formatted_text


# å…¨å±€å®ä¾‹
search_helper = SearchHelper()
